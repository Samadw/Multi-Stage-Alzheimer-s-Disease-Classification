{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Necessary Labries"
      ],
      "metadata": {
        "id": "f3NZeLwl3VPp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKe2IrNZ3Jvn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "from transformers import SegformerForSemanticSegmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Transformation like (resize images, convert to tensor, normalize)\n"
      ],
      "metadata": {
        "id": "SaaaV7L73b7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # resize the images\n",
        "    transforms.ToTensor(),          # convert images to tensor\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # normalize the images\n",
        "])"
      ],
      "metadata": {
        "id": "T-r6tW553SyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path to the train and test datasets\n"
      ],
      "metadata": {
        "id": "evfKHEhj3ovM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = \"dataset\"\n",
        "test_data_path = \"dataset\""
      ],
      "metadata": {
        "id": "d_GzydP_3hBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load train and test datasets using ImageFolder, which automatically assigns labels based on folder names\n"
      ],
      "metadata": {
        "id": "5ym8MFsV347y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.ImageFolder(root=train_data_path, transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root=test_data_path, transform=transform)"
      ],
      "metadata": {
        "id": "tL0YZ7Cm3kSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create data loaders\n"
      ],
      "metadata": {
        "id": "VY7FDVRa372b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "tyWpSYyw3tpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping of class labels (folder names) to indices\n"
      ],
      "metadata": {
        "id": "-gaLjP5q3-wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_dataset.classes  # Should be ['CN', 'EMCI', 'LMCI', 'MCI', 'AD']"
      ],
      "metadata": {
        "id": "JbaXwa9x3wMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check dataset structure and classes\n"
      ],
      "metadata": {
        "id": "vj91ZzhU4BM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Class names: {class_names}\")\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of testing samples: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "Sr-iS6xg3zwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Access first batch of train data\n"
      ],
      "metadata": {
        "id": "EUoiruPV4Dz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in train_loader:\n",
        "    print(f\"Image batch shape: {images.size()}\")\n",
        "    print(f\"Labels: {labels}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "KhCdgDca32mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-Net Model Definition\n"
      ],
      "metadata": {
        "id": "EHx23nhc5Sjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# U-Net Model for Padding and Enhancement (pretrained or custom model)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        def CBR(in_channels, out_channels):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            )\n",
        "\n",
        "        self.encoder1 = CBR(in_channels, 64)\n",
        "        self.encoder2 = CBR(64, 128)\n",
        "        self.encoder3 = CBR(128, 256)\n",
        "        self.encoder4 = CBR(256, 512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = CBR(512, 1024)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.decoder4 = CBR(1024, 512)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.decoder3 = CBR(512, 256)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.decoder2 = CBR(256, 128)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.decoder1 = CBR(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.encoder1(x)\n",
        "        e2 = self.encoder2(self.pool(e1))\n",
        "        e3 = self.encoder3(self.pool(e2))\n",
        "        e4 = self.encoder4(self.pool(e3))\n",
        "\n",
        "        b = self.bottleneck(self.pool(e4))\n",
        "\n",
        "        d4 = self.upconv4(b)\n",
        "        d4 = torch.cat((e4, d4), dim=1)\n",
        "        d4 = self.decoder4(d4)\n",
        "\n",
        "        d3 = self.upconv3(d4)\n",
        "        d3 = torch.cat((e3, d3), dim=1)\n",
        "        d3 = self.decoder3(d3)\n",
        "\n",
        "        d2 = self.upconv2(d3)\n",
        "        d2 = torch.cat((e2, d2), dim=1)\n",
        "        d2 = self.decoder2(d2)\n",
        "\n",
        "        d1 = self.upconv1(d2)\n",
        "        d1 = torch.cat((e1, d1), dim=1)\n",
        "        d1 = self.decoder1(d1)\n",
        "\n",
        "        return self.final_conv(d1)\n"
      ],
      "metadata": {
        "id": "567dJ9i64yM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize U-Net\n"
      ],
      "metadata": {
        "id": "41JUoTap5eGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNet(in_channels=3, out_channels=3)"
      ],
      "metadata": {
        "id": "uRfO2Ib25atw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define transformation for dataset (resize and normalize)\n"
      ],
      "metadata": {
        "id": "UtGDKIBm5kMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "NgN53u1T5hyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define loss function and optimizer\n"
      ],
      "metadata": {
        "id": "1CpGHSVe55Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()  # Pixel-wise comparison using MSE loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "k-6MGMIA5z9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, _ in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, images)  # Loss comparing input and output images\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}\")"
      ],
      "metadata": {
        "id": "lvWWami058H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the pretrained U-Net model (optional)\n",
        "torch.save(model.state_dict(), 'unet_pretrained.pth')"
      ],
      "metadata": {
        "id": "5CmDTAFt5_h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch model to evaluation mode for enhancement\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "0Iur-8nu6CuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to enhance an image using U-Net\n",
        "def enhance_image(image):\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0)  # Add batch dimension\n",
        "        output = model(image)  # Apply U-Net for padding\n",
        "        return output.squeeze(0)  # Remove batch dimension"
      ],
      "metadata": {
        "id": "YjoDlNXN6F6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply U-Net enhancement on both training and testing datasets\n"
      ],
      "metadata": {
        "id": "sGS0HhnL6OZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_display(loader, dataset_type):\n",
        "    for images, _ in loader:\n",
        "        image = images[0]  # Select the first image from the batch\n",
        "        enhanced_image = enhance_image(image)\n",
        "\n",
        "        # Convert tensors to numpy arrays for visualization\n",
        "        original_image = image.permute(1, 2, 0).cpu().numpy()\n",
        "        enhanced_image = enhanced_image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "        # Plot original vs enhanced image\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        ax[0].imshow(np.clip(original_image, 0, 1))\n",
        "        ax[0].set_title(f\"Original Image ({dataset_type})\")\n",
        "\n",
        "        ax[1].imshow(np.clip(enhanced_image, 0, 1))\n",
        "        ax[1].set_title(f\"Enhanced Image ({dataset_type})\")\n",
        "\n",
        "        plt.show()\n",
        "        break  # Display only one image"
      ],
      "metadata": {
        "id": "FW_nY_9g6I_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhance and display images from both training and testing datasets\n"
      ],
      "metadata": {
        "id": "S_5rwYyk6TwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing Training Dataset:\")\n",
        "process_and_display(train_loader, \"Training\")\n",
        "\n",
        "print(\"Processing Testing Dataset:\")\n",
        "process_and_display(test_loader, \"Testing\")"
      ],
      "metadata": {
        "id": "2fzC9pUn6R6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the pretrained MDXNet model for noise reduction\n"
      ],
      "metadata": {
        "id": "NAvnrCSFFa5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_reduction_model = MDXNet(in_channels=3, out_channels=3)"
      ],
      "metadata": {
        "id": "ru1wU16hFWAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the pretrained weights\n"
      ],
      "metadata": {
        "id": "gzW_JEnbFcw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_reduction_model.load_state_dict(torch.load('mdxnet_pretrained.h5'))"
      ],
      "metadata": {
        "id": "RgF9vOSpFZlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Switch MDXNet model to evaluation mode\n"
      ],
      "metadata": {
        "id": "JIW10cE8FpNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noise_reduction_model.eval()"
      ],
      "metadata": {
        "id": "QO6IeYaaFm4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to reduce noise using the pretrained MDXNet\n"
      ],
      "metadata": {
        "id": "Ud8V1qUfFxd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_noise(image):\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0)  # Add batch dimension\n",
        "        output = noise_reduction_model(image)  # Apply MDXNet for noise reduction\n",
        "        return output.squeeze(0)  # Remove batch dimension\n"
      ],
      "metadata": {
        "id": "oOogrE-VFvCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modify the process_and_display function to include noise reduction using the pretrained MDXNet\n"
      ],
      "metadata": {
        "id": "RkmPEnazF7yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_display_with_noise_reduction(loader, dataset_type):\n",
        "    for images, _ in loader:\n",
        "        image = images[0]  # Select the first image from the batch\n",
        "\n",
        "        # Step 1: Apply noise reduction using pretrained MDXNet\n",
        "        denoised_image = reduce_noise(image)\n",
        "\n",
        "        # Step 2: Apply U-Net enhancement on the denoised image\n",
        "        enhanced_image = enhance_image(denoised_image)\n",
        "\n",
        "        # Convert tensors to numpy arrays for visualization\n",
        "        original_image = image.permute(1, 2, 0).cpu().numpy()  # Original image\n",
        "        denoised_image_np = denoised_image.permute(1, 2, 0).cpu().numpy()  # Denoised image\n",
        "        enhanced_image_np = enhanced_image.permute(1, 2, 0).cpu().numpy()  # Enhanced image\n",
        "\n",
        "        # Plot original, denoised, and enhanced images\n",
        "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        ax[0].imshow(np.clip(original_image, 0, 1))\n",
        "        ax[0].set_title(f\"Original Image ({dataset_type})\")\n",
        "\n",
        "        ax[1].imshow(np.clip(denoised_image_np, 0, 1))\n",
        "        ax[1].set_title(f\"Denoised Image ({dataset_type})\")\n",
        "\n",
        "        ax[2].imshow(np.clip(enhanced_image_np, 0, 1))\n",
        "        ax[2].set_title(f\"Enhanced Image ({dataset_type})\")\n",
        "\n",
        "        plt.show()\n",
        "        break  # Display only one image\n"
      ],
      "metadata": {
        "id": "KLrA2IjhF0g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Results"
      ],
      "metadata": {
        "id": "1EW6e8-9GJ2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing Training Dataset with Pretrained MDXNet for Noise Reduction:\")\n",
        "process_and_display_with_noise_reduction(train_loader, \"Training\")\n",
        "\n",
        "print(\"Processing Testing Dataset with Pretrained MDXNet for Noise Reduction:\")\n",
        "process_and_display_with_noise_reduction(test_loader, \"Testing\")\n"
      ],
      "metadata": {
        "id": "nqg7NJATGIfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SRGAN for Image Sharpness Enhancement\n"
      ],
      "metadata": {
        "id": "v3nYr8KhHHAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SRGAN(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3):\n",
        "        super(SRGAN, self).__init__()\n",
        "\n",
        "        # Define a simple upscaling network to enhance sharpness\n",
        "        self.upscale = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.upscale(x)"
      ],
      "metadata": {
        "id": "kOmDwpQjHEkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the pretrained SRGAN model\n"
      ],
      "metadata": {
        "id": "4-8-K5HYHO8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sharpness_model = SRGAN(in_channels=3, out_channels=3)\n",
        "sharpness_model.load_state_dict(torch.load('srgan_pretrained.pth'))  # Pretrained model path\n",
        "sharpness_model.eval()"
      ],
      "metadata": {
        "id": "iM_2AhARHMdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to enhance sharpness using SRGAN\n"
      ],
      "metadata": {
        "id": "9WqU9HpfHUdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enhance_sharpness(image):\n",
        "    with torch.no_grad():\n",
        "        image = image.unsqueeze(0)  # Add batch dimension\n",
        "        output = sharpness_model(image)  # Apply SRGAN for sharpness enhancement\n",
        "        return output.squeeze(0)  # Remove batch dimension\n"
      ],
      "metadata": {
        "id": "03Nqykz4HR9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to process and display images with noise reduction, U-Net, and SRGAN sharpness enhancement\n"
      ],
      "metadata": {
        "id": "mKsbQai2HduE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_display_full_pipeline(loader, dataset_type):\n",
        "    for images, _ in loader:\n",
        "        image = images[0]  # Select the first image from the batch\n",
        "\n",
        "        # Apply noise reduction using pretrained MDXNet\n",
        "        denoised_image = reduce_noise(image)\n",
        "\n",
        "        #  Apply U-Net for padding and enhancement\n",
        "        enhanced_image = enhance_image(denoised_image)\n",
        "\n",
        "        # Apply SRGAN for sharpness enhancement\n",
        "        sharp_image = enhance_sharpness(enhanced_image)\n",
        "\n",
        "        # Convert tensors to numpy arrays for visualization\n",
        "        original_image = image.permute(1, 2, 0).cpu().numpy()\n",
        "        denoised_image = denoised_image.permute(1, 2, 0).cpu().numpy()\n",
        "        enhanced_image = enhanced_image.permute(1, 2, 0).cpu().numpy()\n",
        "        sharp_image = sharp_image.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "        # Plot original vs denoised vs enhanced vs sharp image\n",
        "        fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
        "        ax[0].imshow(np.clip(original_image, 0, 1))\n",
        "        ax[0].set_title(f\"Original Image ({dataset_type})\")\n",
        "\n",
        "        ax[1].imshow(np.clip(denoised_image, 0, 1))\n",
        "        ax[1].set_title(f\"Denoised Image ({dataset_type})\")\n",
        "\n",
        "        ax[2].imshow(np.clip(enhanced_image, 0, 1))\n",
        "        ax[2].set_title(f\"Enhanced Image ({dataset_type})\")\n",
        "\n",
        "        ax[3].imshow(np.clip(sharp_image, 0, 1))\n",
        "        ax[3].set_title(f\"Sharp Image ({dataset_type})\")\n",
        "\n",
        "        plt.show()\n",
        "        break  # Display only one image\n"
      ],
      "metadata": {
        "id": "T1iiftisHbjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply the updated pipeline on training and testing datasets\n"
      ],
      "metadata": {
        "id": "nBl1-sN3HqgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing Training Dataset with Full Pipeline:\")\n",
        "process_and_display_full_pipeline(train_loader, \"Training\")\n",
        "\n",
        "print(\"Processing Testing Dataset with Full Pipeline:\")\n",
        "process_and_display_full_pipeline(test_loader, \"Testing\")"
      ],
      "metadata": {
        "id": "RY6LfN-5HpF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define RNet30 model for classification\n"
      ],
      "metadata": {
        "id": "QDiGjHChIddR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNet30(nn.Module):\n",
        "    def __init__(self, num_classes=5):  #  5 classes: CN, EMCI, LMCI, MCI, AD\n",
        "        super(RNet30, self).__init__()\n",
        "\n",
        "        # Define convolutional layers\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Flatten the output for fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Forward pass through fully connected layers\n",
        "        x = self.fc1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.fc2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "sMJgGMq6IZiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the RNet30 model\n"
      ],
      "metadata": {
        "id": "RTxSrnq9ImvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5  # Number of output classes (CN, EMCI, LMCI, MCI, AD)\n",
        "rnet_model = RNet30(num_classes=num_classes)"
      ],
      "metadata": {
        "id": "A8YZS9FgIj3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define loss function and optimizer\n"
      ],
      "metadata": {
        "id": "E6YVsxoSIxHW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "optimizer = optim.Adam(rnet_model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "C2POkz0jIpk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop for RNet30\n"
      ],
      "metadata": {
        "id": "MNbBUR-AIyv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    rnet_model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = rnet_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader)}, Accuracy: {accuracy}%\")\n",
        "\n",
        "# Save the trained RNet30 model\n",
        "torch.save(rnet_model.state_dict(), 'rnet30_trained')\n",
        "\n",
        "# Switch to evaluation mode for testing\n",
        "rnet_model.eval()\n",
        "\n",
        "# Testing loop for RNet30\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = rnet_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate accuracy on the test dataset\n",
        "test_accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy of RNet30: {test_accuracy}%\")"
      ],
      "metadata": {
        "id": "fNYl6ulxIuh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load pretrained SegFormer model\n"
      ],
      "metadata": {
        "id": "Og-FgOloJh9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segformer = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')"
      ],
      "metadata": {
        "id": "Dl-CRag2JVrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Switch to evaluation mode\n"
      ],
      "metadata": {
        "id": "XTSRjeI5JkkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "segformer.eval()"
      ],
      "metadata": {
        "id": "P95gkkBvJghN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to apply SegFormer for segmentation of affected regions\n"
      ],
      "metadata": {
        "id": "7lQ1fdLWJ1FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_brain_region(image):\n",
        "    with torch.no_grad():\n",
        "        # Preprocess the image for SegFormer\n",
        "        image = image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Apply SegFormer for segmentation\n",
        "        outputs = segformer(image)['logits']  # Get segmentation logits\n",
        "        segmentation_map = torch.argmax(outputs, dim=1)  # Get predicted class for each pixel\n",
        "\n",
        "        return segmentation_map.squeeze(0)  # Remove batch dimension\n"
      ],
      "metadata": {
        "id": "iPfIljKKJxjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Updated pipeline for segmentation after classification\n"
      ],
      "metadata": {
        "id": "jW6s68RgJ-ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_and_segment(loader, dataset_type):\n",
        "    for images, labels in loader:\n",
        "        image = images[0]  # Select the first image from the batch\n",
        "\n",
        "        # Step 1: Classify using RNet30\n",
        "        rnet_model.eval()  # Switch to evaluation mode\n",
        "        outputs = rnet_model(image.unsqueeze(0))  # Add batch dimension\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # If the prediction indicates an affected brain region, proceed with segmentation\n",
        "        if predicted.item() in [1, 2, 3, 4]:  #  1 to 4 represent EMCI, LMCI, MCI, SD\n",
        "            print(f\"Detected affected region in {dataset_type} image. Applying SegFormer for segmentation...\")\n",
        "\n",
        "            # Step 2: Apply SegFormer for segmentation\n",
        "            segmentation_map = segment_brain_region(image)\n",
        "\n",
        "            # Convert tensors to numpy arrays for visualization\n",
        "            original_image = image.permute(1, 2, 0).cpu().numpy()\n",
        "            segmentation_map = segmentation_map.cpu().numpy()\n",
        "\n",
        "            # Plot original vs segmented image\n",
        "            fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            ax[0].imshow(np.clip(original_image, 0, 1))\n",
        "            ax[0].set_title(f\"Original Image ({dataset_type})\")\n",
        "\n",
        "            ax[1].imshow(segmentation_map, cmap='gray')\n",
        "            ax[1].set_title(f\"Segmented Affected Region ({dataset_type})\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            print(f\"No affected region detected in {dataset_type} image.\")\n",
        "\n",
        "        break  # Process only one image"
      ],
      "metadata": {
        "id": "0e4_e-cMJ8pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply the full pipeline on training and testing datasets\n"
      ],
      "metadata": {
        "id": "YaHQe8i9KD3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing and Segmenting Training Dataset:\")\n",
        "process_and_segment(train_loader, \"Training\")\n",
        "\n",
        "print(\"Processing and Segmenting Testing Dataset:\")\n",
        "process_and_segment(test_loader, \"Testing\")"
      ],
      "metadata": {
        "id": "m6JRx5EKKCKp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}